import os
import glob
import math
import time
from dataclasses import dataclass
from typing import List, Tuple, Dict, Optional

import numpy as np
import pandas as pd
import streamlit as st
from PIL import Image

import torch
import open_clip


# -----------------------------
# App Config
# -----------------------------
st.set_page_config(
    page_title="Traffic Sign CLIP Zero-shot Classifier",
    layout="wide",
)

# -----------------------------
# Utilities
# -----------------------------
def get_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device("cuda")
    # Apple Silicon (M1/M2/M3/M4) í™˜ê²½ì´ë©´ ì•„ë˜ê°€ Trueì¼ ìˆ˜ ìˆìŒ
    if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def cosine_sim(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """a: [N, D], b: [M, D] -> sim: [N, M]"""
    a = a / (a.norm(dim=-1, keepdim=True) + 1e-8)
    b = b / (b.norm(dim=-1, keepdim=True) + 1e-8)
    return a @ b.T


def softmax(x: np.ndarray) -> np.ndarray:
    x = x - np.max(x)
    e = np.exp(x)
    return e / (np.sum(e) + 1e-12)


def load_image(pil_img: Image.Image) -> Image.Image:
    if pil_img.mode != "RGB":
        pil_img = pil_img.convert("RGB")
    return pil_img


# -----------------------------
# Default Prompt Set (Traffic Signs)
# í•„ìš”ì— ë§ê²Œ ëŠ˜ë¦¬ë©´ ë¨
# -----------------------------
DEFAULT_CLASSES = [
    ("stop", [
        "a photo of a red octagonal stop sign",
        "a traffic sign that says STOP",
        "a stop sign on the road",
    ]),
    ("speed_limit", [
        "a photo of a speed limit sign",
        "a circular speed limit sign with a number",
        "a road sign indicating speed limit",
    ]),
    ("no_entry", [
        "a photo of a no entry sign",
        "a traffic sign with a red circle and a white horizontal bar",
        "a do not enter road sign",
    ]),
    ("yield", [
        "a photo of a yield sign",
        "a triangular yield sign",
        "a give way road sign",
    ]),
    ("pedestrian_crossing", [
        "a photo of a pedestrian crossing sign",
        "a road sign indicating pedestrian crossing",
        "a crosswalk warning sign",
    ]),
    ("traffic_light_ahead", [
        "a photo of a traffic light ahead warning sign",
        "a road sign indicating traffic signal ahead",
        "a sign warning of a traffic signal",
    ]),
    ("school_zone", [
        "a photo of a school zone sign",
        "a road sign indicating school zone",
        "a sign warning drivers to slow down near a school",
    ]),
    ("no_parking", [
        "a photo of a no parking sign",
        "a road sign indicating parking is not allowed",
        "a no parking traffic sign",
    ]),
    ("u_turn_prohibited", [
        "a photo of a no u-turn sign",
        "a road sign prohibiting u-turn",
        "a traffic sign with a u-turn arrow crossed out",
    ]),
    ("one_way", [
        "a photo of a one way sign",
        "a road sign indicating one-way traffic",
        "a one way arrow traffic sign",
    ]),
]

# í•œêµ­ ë„ë¡œí‘œì§€ì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ë¥¼ í•œêµ­ì–´ë¡œë„ ë³´ê°•í•˜ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì— ì¶”ê°€í•˜ì„¸ìš”.
KOREAN_HINTS = {
    "stop": ["ì •ì§€ í‘œì§€íŒ ì‚¬ì§„", "ë„ë¡œ ì •ì§€ í‘œì§€íŒ"],
    "speed_limit": ["ì œí•œì†ë„ í‘œì§€íŒ ì‚¬ì§„", "ì†ë„ ì œí•œ í‘œì§€"],
    "no_entry": ["ì§„ì…ê¸ˆì§€ í‘œì§€íŒ ì‚¬ì§„", "ì¶œì… ê¸ˆì§€ í‘œì§€"],
    "yield": ["ì–‘ë³´ í‘œì§€íŒ ì‚¬ì§„", "ì„œí–‰ ì–‘ë³´ í‘œì§€"],
    "pedestrian_crossing": ["íš¡ë‹¨ë³´ë„ í‘œì§€íŒ ì‚¬ì§„", "ë³´í–‰ì íš¡ë‹¨ ì£¼ì˜ í‘œì§€"],
    "traffic_light_ahead": ["ì‹ í˜¸ë“± ì£¼ì˜ í‘œì§€íŒ ì‚¬ì§„", "ì „ë°© ì‹ í˜¸ë“± í‘œì§€"],
    "school_zone": ["ì–´ë¦°ì´ ë³´í˜¸êµ¬ì—­ í‘œì§€íŒ ì‚¬ì§„", "ìŠ¤ì¿¨ì¡´ í‘œì§€"],
    "no_parking": ["ì£¼ì°¨ê¸ˆì§€ í‘œì§€íŒ ì‚¬ì§„", "ì£¼ì •ì°¨ ê¸ˆì§€ í‘œì§€"],
    "u_turn_prohibited": ["ìœ í„´ê¸ˆì§€ í‘œì§€íŒ ì‚¬ì§„", "ìœ í„´ ê¸ˆì§€ í‘œì§€"],
    "one_way": ["ì¼ë°©í†µí–‰ í‘œì§€íŒ ì‚¬ì§„", "ì¼ë°©í†µí–‰ í‘œì§€"],
}

# -----------------------------
# Model Loader (cached)
# -----------------------------
@st.cache_resource(show_spinner=False)
def load_clip_model(model_name: str, pretrained: str, device_str: str):
    device = torch.device(device_str)
    model, _, preprocess = open_clip.create_model_and_transforms(
        model_name=model_name,
        pretrained=pretrained,
    )
    tokenizer = open_clip.get_tokenizer(model_name)
    model.eval().to(device)
    return model, preprocess, tokenizer


def build_text_features(
    model,
    tokenizer,
    class_prompts: Dict[str, List[str]],
    device: torch.device,
    normalize: bool = True,
) -> Tuple[List[str], torch.Tensor]:
    """
    class_prompts: {label: [prompt1, prompt2, ...]}
    return:
      labels: ["stop", "speed_limit", ...]
      text_feats: [num_labels, D]  (ê° ë¼ë²¨ì˜ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í‰ê· )
    """
    labels = []
    feats = []
    with torch.no_grad():
        for label, prompts in class_prompts.items():
            tokens = tokenizer(prompts).to(device)
            tf = model.encode_text(tokens)  # [P, D]
            if normalize:
                tf = tf / (tf.norm(dim=-1, keepdim=True) + 1e-8)
            tf_mean = tf.mean(dim=0, keepdim=True)  # [1, D]
            if normalize:
                tf_mean = tf_mean / (tf_mean.norm(dim=-1, keepdim=True) + 1e-8)
            labels.append(label)
            feats.append(tf_mean)
    text_feats = torch.cat(feats, dim=0)  # [L, D]
    return labels, text_feats


def predict_image(
    model,
    preprocess,
    image: Image.Image,
    text_labels: List[str],
    text_feats: torch.Tensor,
    device: torch.device,
    topk: int = 5,
) -> List[Tuple[str, float]]:
    img = load_image(image)
    img_t = preprocess(img).unsqueeze(0).to(device)  # [1, C, H, W]
    with torch.no_grad():
        img_feat = model.encode_image(img_t)  # [1, D]
        img_feat = img_feat / (img_feat.norm(dim=-1, keepdim=True) + 1e-8)

        sims = (img_feat @ text_feats.T).squeeze(0)  # [L]
        sims_np = sims.detach().float().cpu().numpy()
        probs = softmax(sims_np)  # pseudo-probabilities

    idx = np.argsort(-probs)[:topk]
    return [(text_labels[i], float(probs[i])) for i in idx]


# -----------------------------
# UI
# -----------------------------
st.title("ğŸš¦ Traffic Sign CLIP Zero-shot Classifier (Streamlit)")
st.caption("ì´ë¯¸ì§€ í•œ ì¥ ì—…ë¡œë“œ â†’ CLIPìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„ë¥˜ / í´ë” ì¼ê´„ ì˜ˆì¸¡ ì§€ì›")

with st.sidebar:
    st.subheader("âš™ï¸ Model")
    # ê°€ë³ê³  ì‹¤ìš©ì ì¸ ì¡°í•© (CPUì—ì„œë„ ê·¸ë‚˜ë§ˆ ì“¸ë§Œ)
    model_name = st.selectbox(
        "CLIP ëª¨ë¸",
        ["ViT-B-32", "ViT-B-16", "ViT-L-14"],
        index=0,
    )
    pretrained = st.selectbox(
        "Pretrained",
        # open_clipì—ì„œ ìì£¼ ì“°ëŠ” í”„ë¦¬íŠ¸ë ˆì¸
        ["openai", "laion2b_s34b_b79k", "laion2b_s32b_b82k"],
        index=0,
    )

    device = get_device()
    st.write(f"ğŸ–¥ï¸ Device: **{device.type}**")

    st.divider()
    st.subheader("ğŸ§  Prompt / Classes")
    use_korean = st.checkbox("í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë„ í•¨ê»˜ ì‚¬ìš©", value=True)

    # ê¸°ë³¸ í´ë˜ìŠ¤ ë¡œë“œ
    class_prompts = {}
    for label, prompts in DEFAULT_CLASSES:
        merged = list(prompts)
        if use_korean and label in KOREAN_HINTS:
            merged += KOREAN_HINTS[label]
        class_prompts[label] = merged

    # ì‚¬ìš©ì ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì¶”ê°€
    st.caption("ì›í•˜ë©´ í´ë˜ìŠ¤/í”„ë¡¬í”„íŠ¸ë¥¼ ì§ì ‘ ì¶”ê°€í•  ìˆ˜ ìˆì–´ìš”.")
    custom_block = st.text_area(
        "ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ (í˜•ì‹: label|prompt1;prompt2;prompt3)",
        value="",
        placeholder="ì˜ˆ)\nparking|a photo of a parking sign; a road sign indicating parking\nwarning|a triangular warning road sign",
        height=120,
    )

    if custom_block.strip():
        for line in custom_block.splitlines():
            line = line.strip()
            if not line or "|" not in line:
                continue
            label, prompts_str = line.split("|", 1)
            label = label.strip()
            prompts = [p.strip() for p in prompts_str.split(";") if p.strip()]
            if label and prompts:
                class_prompts[label] = prompts

    topk = st.slider("Top-K", 1, 10, 5)

# Load model
with st.spinner("CLIP ëª¨ë¸ ë¡œë”© ì¤‘..."):
    model, preprocess, tokenizer = load_clip_model(model_name, pretrained, str(device))

# Build text features (cached by Streamlit session, but depends on prompts; keep it simple)
with st.spinner("í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘..."):
    text_labels, text_feats = build_text_features(model, tokenizer, class_prompts, device)

tab1, tab2 = st.tabs(["ğŸ–¼ï¸ ë‹¨ì¼ ì´ë¯¸ì§€", "ğŸ“ í´ë” ì¼ê´„ ì˜ˆì¸¡"])

with tab1:
    colL, colR = st.columns([1, 1], gap="large")

    with colL:
        uploaded = st.file_uploader("êµí†µí‘œì§€íŒ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "jpeg", "png", "webp"])
        if uploaded:
            img = Image.open(uploaded)
            st.image(img, caption="ì…ë ¥ ì´ë¯¸ì§€", use_container_width=True)

    with colR:
        st.subheader("ì˜ˆì¸¡ ê²°ê³¼")
        if uploaded:
            preds = predict_image(
                model=model,
                preprocess=preprocess,
                image=img,
                text_labels=text_labels,
                text_feats=text_feats,
                device=device,
                topk=topk,
            )
            df = pd.DataFrame(preds, columns=["label", "score"])
            st.dataframe(df, use_container_width=True, hide_index=True)

            best_label, best_score = preds[0]
            st.success(f"âœ… ì˜ˆì¸¡: **{best_label}**  (score={best_score:.4f})")

            st.caption(
                "â€» scoreëŠ” softmaxë¡œ ë§Œë“  'ìœ ì‚¬ë„ ê¸°ë°˜ í™•ë¥ ì²˜ëŸ¼ ë³´ì´ëŠ” ê°’'ì…ë‹ˆë‹¤. "
                "ì •í™•í•œ í™•ë¥ ì´ë¼ê¸°ë³´ë‹¤ í´ë˜ìŠ¤ ê°„ ìƒëŒ€ ë¹„êµìš©ìœ¼ë¡œ ë³´ì„¸ìš”."
            )
        else:
            st.info("ì™¼ìª½ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ ê²°ê³¼ê°€ ë‚˜ì˜µë‹ˆë‹¤.")

with tab2:
    st.subheader("ğŸ“ í´ë” ë‚´ ì´ë¯¸ì§€ ì¼ê´„ ì˜ˆì¸¡")
    st.caption("ë¡œì»¬ í´ë” ê²½ë¡œë¥¼ ë„£ìœ¼ë©´ JPG/PNG ë“±ì„ í›‘ì–´ì„œ Top-1 ê²°ê³¼ë¥¼ CSVë¡œ ë‚´ë ¤ë°›ì„ ìˆ˜ ìˆì–´ìš”.")

    folder = st.text_input("ì´ë¯¸ì§€ í´ë” ê²½ë¡œ", value="", placeholder=r"C:\data\traffic_signs\images")
    exts = st.multiselect("í™•ì¥ì", ["jpg", "jpeg", "png", "webp"], default=["jpg", "jpeg", "png"])

    run = st.button("ì¼ê´„ ì˜ˆì¸¡ ì‹¤í–‰", type="primary")

    if run:
        if not folder.strip():
            st.error("í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        elif not os.path.isdir(folder):
            st.error("í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            patterns = []
            for e in exts:
                patterns.append(os.path.join(folder, f"**/*.{e}"))
                patterns.append(os.path.join(folder, f"**/*.{e.upper()}"))
            files = []
            for p in patterns:
                files.extend(glob.glob(p, recursive=True))
            files = sorted(list(set(files)))

            if not files:
                st.warning("í•´ë‹¹ í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.write(f"ì´ íŒŒì¼ ìˆ˜: **{len(files)}**")
                rows = []
                prog = st.progress(0)
                t0 = time.time()

                for i, fp in enumerate(files, start=1):
                    try:
                        im = Image.open(fp)
                        pred1 = predict_image(
                            model=model,
                            preprocess=preprocess,
                            image=im,
                            text_labels=text_labels,
                            text_feats=text_feats,
                            device=device,
                            topk=1,
                        )[0]
                        rows.append({
                            "path": fp,
                            "pred_label": pred1[0],
                            "score": pred1[1],
                        })
                    except Exception as e:
                        rows.append({
                            "path": fp,
                            "pred_label": "ERROR",
                            "score": np.nan,
                            "error": str(e),
                        })

                    prog.progress(i / len(files))

                dt = time.time() - t0
                df = pd.DataFrame(rows)
                st.dataframe(df.head(50), use_container_width=True)

                st.success(f"ì™„ë£Œ âœ… (ì²˜ë¦¬ì‹œê°„: {dt:.2f}s, í‰ê·  {dt/len(files):.4f}s/ì¥)")

                csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    "CSV ë‹¤ìš´ë¡œë“œ",
                    data=csv_bytes,
                    file_name="clip_batch_predictions.csv",
                    mime="text/csv",
                )

st.divider()
st.caption(
    "íŒ) í•œêµ­ êµí†µí‘œì§€íŒì€ í˜•íƒœ/ìƒ‰ìƒ ë‹¨ì„œê°€ ì¤‘ìš”í•´ì„œ, "
    "í”„ë¡¬í”„íŠ¸ì— 'red circle', 'blue circle', 'triangle', 'octagon', 'speed limit number' ê°™ì€ ë‹¨ì„œë¥¼ ë„£ìœ¼ë©´ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤."
)
