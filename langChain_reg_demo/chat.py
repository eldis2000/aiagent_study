import os

from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

INDEX_DIR = "./faiss_index"
LLM_MODEL = "qwen2:7b"
EMBED_MODEL = "nomic-embed-text"


def main():
    if not os.path.exists(INDEX_DIR):
        raise RuntimeError("âŒ FAISS ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. build_index.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    vectorstore = FAISS.load_local(
        INDEX_DIR,
        embeddings,
        allow_dangerous_deserialization=True
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    llm = ChatOllama(
        model=LLM_MODEL,
        temperature=0.2
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë„ˆëŠ” ë¬¸ì„œ ê¸°ë°˜ Q&A ì±—ë´‡ì´ë‹¤. "
         "ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ë‹µí•´ë¼. "
         "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ 'ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•´ë¼."),
        ("human",
         "ì§ˆë¬¸: {question}\n\n"
         "ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
         "ë‹µë³€:")
    ])

    def format_docs(docs):
        return "\n\n---\n\n".join(d.page_content for d in docs)

    chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
    )

    print("ğŸ¤– ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡ ì‹¤í–‰ (exit / quit ì¢…ë£Œ)")
    while True:
        q = input("\nYou: ").strip()
        if q.lower() in ("exit", "quit"):
            break

        resp = chain.invoke(q)
        print(f"Bot: {resp.content}")


if __name__ == "__main__":
    main()
