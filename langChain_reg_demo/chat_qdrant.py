from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain_community.chat_models import ChatOllama

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

from qdrant_client import QdrantClient

COLLECTION_NAME = "doc_knowledge_base"
QDRANT_URL = "http://localhost:6333"

LLM_MODEL = "llama3"          # ollama list ê²°ê³¼ì™€ ì¼ì¹˜
EMBED_MODEL = "nomic-embed-text"


def main():
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    client = QdrantClient(url=QDRANT_URL)

    vectorstore = Qdrant(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    llm = ChatOllama(
        model=LLM_MODEL,
        temperature=0.2
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë„ˆëŠ” ë¬¸ì„œ ê¸°ë°˜ Q&A ì±—ë´‡ì´ë‹¤. "
         "ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë§Œ ê·¼ê±°ë¡œ ë‹µí•´ë¼. "
         "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ 'ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•´ë¼."),
        ("human",
         "ì§ˆë¬¸: {question}\n\n"
         "ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
         "ë‹µë³€:")
    ])

    def format_docs(docs):
        return "\n\n---\n\n".join(d.page_content for d in docs)

    chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
    )

    print("ğŸ¤– Qdrant ê¸°ë°˜ ë¬¸ì„œ ì±—ë´‡ ì‹¤í–‰ (exit / quit ì¢…ë£Œ)")
    while True:
        q = input("\nYou: ").strip()
        if q.lower() in ("exit", "quit"):
            break

        resp = chain.invoke(q)
        print(f"Bot: {resp.content}")


if __name__ == "__main__":
    main()
